---
title: Jobs
icon: diagram-project
---

<Accordion title="What is a Job?">
    A job is a specific execution of a workflow with designated input parameters. It consists of one or more tasks that can run in parallel or sequentially, based on their dependencies. Submitting a job involves creating a root task with specific input parameters, which may trigger the execution of other tasks within the same job.
</Accordion>

## Submission

To execute a [task](/workflows/concepts/tasks), it must be initialized with concrete inputs and submitted as a job. The task will then run within the context of the job, and if it generates sub-tasks, those will also execute as part of the same job.

After submitting a job, the root task is scheduled for execution, and any [eligible task runner](/workflows/concepts/task-runners#task-selection) can pick it up and execute it.

First, instantiate a job client by calling the `jobs` method on the workflow client.

<CodeGroup>
```python Python
from tilebox.workflows import Client

client = Client()
job_client = client.jobs()
```
</CodeGroup>

After obtaining a job client, submit a job using the [submit](/api-reference/python/tilebox.workflows/JobClient.submit) method. You need to provide a name for the job, an instance of the root [task](/workflows/concepts/tasks), and a [cluster](/workflows/concepts/clusters) to execute the root task on.

<CodeGroup>
```python Python
# import your own workflow
from my_workflow import MyTask

cluster = "dev-cluster"
job = job_client.submit('my-job', MyTask("some", "parameters"), cluster)
```
</CodeGroup>

Once a job is submitted, it's immediately scheduled for execution. The root task will be picked up and executed as soon as an [eligible task runner](/workflows/concepts/task-runners#task-selection) is available.

## Retry Handling

[Tasks support retry handling](/workflows/concepts/tasks#retry-handling) for failed executions. This applies to the root task of a job as well, where you can specify the number of retries using the `max_retries` argument of the `submit` method.

<CodeGroup>
```python Python
from my_workflow import MyFlakyTask

cluster = "dev-cluster"
job = job_client.submit('my-job', MyFlakyTask(), cluster, max_retries=5)
```
</CodeGroup>

In this example, if `MyFlakyTask` fails, it will be retried up to five times before being marked as failed.

## Retrieving a specific job

When you submit a job, it's assigned a unique identifier that can be used to retrieve it later.

You can use the `find` method on the job client to get a job by its ID.

<CodeGroup title="Retrieving a Job by ID">
```python Python
job = job_client.submit('my-job', MyTask("some", "parameters"), cluster)
print(job.id)  # 018dd029-58ca-74e5-8b58-b4f99d610f9a

# Later, in another process or machine, retrieve job info
job = job_client.find("018dd029-58ca-74e5-8b58-b4f99d610f9a")
```
</CodeGroup>

<Tip>
    `find` is also a useful tool for fetching a jobs state after a while, to check if it's still running or has already completed.
</Tip>

## Visualization

Visualizing the execution of a job can be helpful. The Tilebox workflow orchestrator tracks all tasks in a job, including [sub-tasks](/workflows/concepts/tasks#task-composition-and-subtasks) and [dependencies](/workflows/concepts/tasks#dependencies). This enables the visualization of the execution of a job as a graph diagram.

<Note>
`display` is designed for use in an [interactive environment](/sdks/python/sample-notebooks#interactive-environments) such as a Jupyter notebook. In non-interactive environments, use [visualize](/api-reference/python/tilebox.workflows/JobClient.visualize), which returns the rendered diagram as an SVG string.
</Note>

<CodeGroup>
```python Python
job = job_client.find("some-job-id")  # or a recently submitted job
# Then visualize it
job_client.display(job)
```
</CodeGroup>

The following diagram represents the job execution as a graph. Each task is shown as a node, with edges indicating sub-task relationships. The diagram also uses color coding to display the status of each task.

<Frame>
    <img src="/assets/workflows/diagrams/svg/task-states.svg" alt="Color coding of task states" className="dark:hidden"/>
    <img src="/assets/workflows/diagrams/svg/task-states.dark.svg" alt="Color coding of task states" className="hidden dark:block" />
</Frame>

The color codes for task states are:

| Task State | Color | Description |
|------------|-------|-------------|
| Queued     | <span className="dark:hidden">Salmon</span><span className="hidden dark:block">Yellow</span>  | The task is queued and waiting for execution. |
| Running    | Blue | The task is currently being executed. |
| Computed  | Green | The task has successfully been computed. If a task is computed, and all it's sub-tasks are also computed, the task is considered completed. |
| Failed     | Red | The task has been executed but encountered an error. |

Below is another visualization of a job currently being executed by multiple task runners.

<Frame>
    <img src="/assets/workflows/diagrams/svg/multiple-runners.svg" alt="Job being executed by multiple runners" className="dark:hidden"/>
    <img src="/assets/workflows/diagrams/svg/multiple-runners.dark.svg" alt="Job being executed by multiple runners" className="hidden dark:block" />
</Frame>

This visualization shows:

- The root task, `MyTask`, has executed and spawned three sub-tasks.
- At least three task runners are available, as three tasks currently are executed simultaneously.
- The `SubTask` that is still executing has not generated any sub-tasks yet, as sub-tasks are queued for execution only after the parent task finishes and becomes computed.
- The queued `DependentTask` requires the `LeafTask` to complete before it can be executed.

<Note>
Job visualizations are meant for development and debugging. They are not suitable for large jobs with hundreds of tasks, as the diagrams may become too complex. Currently, visualizations are limited to jobs with a maximum of 200 tasks.
</Note>

### Customizing Task Display Names

The text representing a task in the diagram defaults to a tasks class name. You can customize this by modifying the `display` field of the `current_task` object in the task's execution context. The maximum length for a display name is 1024 characters, with any overflow truncated. Line breaks using `\n` are supported as well.

<CodeGroup>
```python Python
from tilebox.workflows import Task, ExecutionContext

class RootTask(Task):
    num_subtasks: int

    def execute(self, context: ExecutionContext):
        context.current_task.display = f"Root({self.num_subtasks})"
        for i in range(self.num_subtasks):
            context.submit_subtask(SubTask(i))

class SubTask(Task):
    index: int

    def execute(self, context: ExecutionContext):
        context.current_task.display = f"Leaf Nr. {self.index}"

job = job_client.submit('custom-display-names', RootTask(3), "dev-cluster")
job_client.display(job)
```
</CodeGroup>

<Frame>
    <img src="/assets/workflows/diagrams/svg/custom-display-names.svg" alt="Customize Tasks Display Names" className="dark:hidden"/>
    <img src="/assets/workflows/diagrams/svg/custom-display-names.dark.svg" alt="Customize Tasks Display Names" className="hidden dark:block" />
</Frame>

## Cancellation

You can cancel a job at any time. When a job is canceled, no queued tasks will be picked up by task runners and executed even if task runners are idle. Tasks that are already being executed will finish their execution and not be interrupted. All sub-tasks spawned from such tasks after the cancellation will not be picked up by task runners.

Use the `cancel` method on the job client to cancel a job.

<CodeGroup>
```python Python
job = job_client.submit('my-job', MyTask(), "dev-cluster")
# After a short while, the job gets canceled
job_client.cancel(job)
```
</CodeGroup>

<Note>
A canceled job can be resumed at any time by [retrying](#retries) it.
</Note>

If any task in a job fails, the job is automatically canceled to avoid executing irrelevant tasks. Future releases will allow configuring this behavior for each task to meet specific requirements.

## Retries

If a task fails due to a bug or lack of resources, there is no need to resubmit the entire job. You can simply retry the job, and it will resume from the point of failure. This ensures that all the work that was already done up until the point of the failure isn't lost.

<Note>
Future releases may introduce automatic retries for certain failure conditions, which can be useful for handling temporary issues.
</Note>

Below is an example of a failing job due to a bug in the task's implementation. The following workflow processes a list of movie titles and queries the [OMDb API](http://www.omdbapi.com/) for each movie's release date.

<CodeGroup>
```python Python
from urllib.parse import urlencode
import httpx
from tilebox.workflows import Task, ExecutionContext

class MoviesStats(Task):
    titles: list[str]

    def execute(self, context: ExecutionContext) -> None:
        for title in self.titles:
            context.submit_subtask(PrintMovieStats(title))

class PrintMovieStats(Task):
    title: str

    def execute(self, context: ExecutionContext) -> None:
        params = {"t": self.title, "apikey": "<OMDB API Key>"}
        url = "http://www.omdbapi.com/?" + urlencode(params)
        response = httpx.get(url).json()
        # set the display name of the task to the title of the movie:
        context.current_task.display = response["Title"]
        print(f"{response['Title']} was released on {response['Released']}")
```
</CodeGroup>

Submitting the workflow as a job reveals a bug in the `PrintMovieStats` task.

<CodeGroup>
```python Python
job = job_client.submit('movies-stats', MoviesStats([
    "The Matrix",
    "Shrek 2",
    "Tilebox - The Movie",
    "The Avengers",
]), "dev-cluster")

job_client.display(job)
```
</CodeGroup>

<Frame>
    <img src="/assets/workflows/diagrams/svg/movies-failed.svg" alt="Job that failed due to a bug" className="dark:hidden"/>
    <img src="/assets/workflows/diagrams/svg/movies-failed.dark.svg" alt="Job that failed due to a bug" className="hidden dark:block" />
</Frame>

One of the `PrintMovieStats` tasks fails with a `KeyError`. This error occurs when a movie title is not found by the [OMDb API](http://www.omdbapi.com/), leading to a response without the `Title` and `Released` fields.

Console output from the task runners confirms this:

```plaintext Output
The Matrix was released on 31 Mar 1999
Shrek 2 was released on 19 May 2004
ERROR: Task PrintMovieStats failed with exception: KeyError('Title')
```

The corrected version of `PrintMovieStats` is as follows:

<CodeGroup title="Fixed Task Implementation">
```python Python
class PrintMovieStats(Task):
    title: str

    def execute(self, context: ExecutionContext) -> None:
        params = {"t": self.title, "apikey": "<OMDB API Key>"}
        url = "http://www.omdbapi.com/?" + urlencode(params)
        response = httpx.get(url).json()
        if "Title" in response and "Released" in response:
            context.current_task.display = response["Title"]
            print(f"{response['Title']} was released on {response['Released']}")
        else:
            context.current_task.display = f"NotFound: {self.title}"
            print(f"Could not find the release date for {self.title}")
```
</CodeGroup>

With this fix, and after redeploying the task runners with the updated `PrintMovieStats` implementation, you can retry the job:

<CodeGroup>
```python Python
job_client.retry(job)
job_client.display(job)
```
</CodeGroup>

<Frame>
    <img src="/assets/workflows/diagrams/svg/movies-retried.svg" alt="Job retried successfully" className="dark:hidden"/>
    <img src="/assets/workflows/diagrams/svg/movies-retried.dark.svg" alt="Job retried successfully" className="hidden dark:block" />
</Frame>

Now the console output shows:

```plaintext Output
Could not find the release date for Tilebox - The Movie
The Avengers was released on 04 May 2012
```

<Note>
The output confirms that only two tasks were executed, resuming from the point of failure instead of re-executing all tasks.
</Note>

The job was retried and succeeded. The two tasks that completed before the failure were not re-executed.
